{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom collections import deque, namedtuple\nimport time\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- AUCTION ENVIRONMENT ----------\n\nclass AuctionEnv:\n    def __init__(self, \n                 max_time_steps=100, \n                 min_item_value=50, \n                 max_item_value=200,\n                 n_snipers=1,\n                 n_incrementalists=1,\n                 n_jump_bidders=1,\n                 min_sniper_value=50,\n                 max_sniper_value=150,\n                 min_incrementalist_value=50,\n                 max_incrementalist_value=150,\n                 min_jump_value=50,\n                 max_jump_value=150):\n        \n        self.max_time_steps = max_time_steps\n        self.min_item_value = min_item_value\n        self.max_item_value = max_item_value\n        \n        # Other bidder counts\n        self.n_snipers = n_snipers\n        self.n_incrementalists = n_incrementalists\n        self.n_jump_bidders = n_jump_bidders\n        \n        # Value ranges for other bidders\n        self.min_sniper_value = min_sniper_value\n        self.max_sniper_value = max_sniper_value\n        self.min_incrementalist_value = min_incrementalist_value\n        self.max_incrementalist_value = max_incrementalist_value\n        self.min_jump_value = min_jump_value\n        self.max_jump_value = max_jump_value\n        \n        # Available bid increments (actions)\n        self.actions = [0, 1, 5, 10]  # 0 = no bid, others are bid increments\n        self.action_space = len(self.actions)\n        \n        # State space components\n        self.state_dim = 3  # time remaining, current price, private value\n        \n        # Initialize the environment\n        self.reset()\n    \n    def reset(self):\n        \"\"\"Reset the auction environment for a new episode.\"\"\"\n        # Initialize auction state\n        self.current_time_step = 0\n        self.current_price = 0\n        self.last_bid_change = 0\n        self.auction_ended = False\n        self.agent_is_winner = False\n        \n        # Generate a random private value for the agent\n        self.agent_private_value = random.uniform(self.min_item_value, self.max_item_value)\n        \n        # Initialize other bidders\n        self.snipers = []\n        self.incrementalists = []\n        self.jump_bidders = []\n        \n        # Create snipers\n        for _ in range(self.n_snipers):\n            value = random.uniform(self.min_sniper_value, self.max_sniper_value)\n            snipe_time = random.randint(self.max_time_steps - 5, self.max_time_steps - 1)\n            self.snipers.append({\n                'value': value,\n                'snipe_time': snipe_time,\n                'has_bid': False\n            })\n        \n        # Create incrementalists\n        for _ in range(self.n_incrementalists):\n            value = random.uniform(self.min_incrementalist_value, self.max_incrementalist_value)\n            bid_frequency = random.randint(5, 15)  # How often they bid\n            bid_increment = random.choice([1, 2, 3])  # How much they increase\n            self.incrementalists.append({\n                'value': value,\n                'bid_frequency': bid_frequency,\n                'bid_increment': bid_increment,\n                'last_bid_time': -bid_frequency  # To ensure they can bid at the start\n            })\n        \n        # Create jump bidders\n        for _ in range(self.n_jump_bidders):\n            value = random.uniform(self.min_jump_value, self.max_jump_value)\n            jump_time = random.randint(int(0.1 * self.max_time_steps), \n                                      int(0.4 * self.max_time_steps))\n            jump_fraction = random.uniform(0.4, 0.7)  # They bid this fraction of their value\n            self.jump_bidders.append({\n                'value': value,\n                'jump_time': jump_time,\n                'jump_fraction': jump_fraction,\n                'has_jumped': False\n            })\n        \n        # Return the initial state\n        return self._get_state()\n    \n    def _get_state(self):\n        \"\"\"Return the current state as a numpy array.\"\"\"\n        normalized_time = (self.max_time_steps - self.current_time_step) / self.max_time_steps\n        normalized_price = self.current_price / self.max_item_value\n        normalized_value = self.agent_private_value / self.max_item_value\n        \n        return np.array([normalized_time, normalized_price, normalized_value])\n    \n    def step(self, action_idx):\n        \"\"\"Take an action in the environment and return next state, reward, done.\"\"\"\n        if self.auction_ended:\n            return self._get_state(), 0, True, {}\n        \n        # Process the agent's action\n        bid_increment = self.actions[action_idx]\n        \n        # If the agent bids\n        if bid_increment > 0:\n            potential_bid = self.current_price + bid_increment\n            \n            # Only update if the agent's bid is higher than current price\n            if potential_bid > self.current_price:\n                self.last_bid_change = bid_increment\n                self.current_price = potential_bid\n                self.agent_is_winner = True\n        \n        # Process other bidders' actions\n        self._process_other_bidders()\n        \n        # Increment time step\n        self.current_time_step += 1\n        \n        # Check if auction has ended\n        done = self.current_time_step >= self.max_time_steps\n        \n        # Calculate reward (only at the end of auction)\n        reward = 0\n        if done:\n            self.auction_ended = True\n            if self.agent_is_winner:\n                reward = self.agent_private_value - self.current_price\n        \n        return self._get_state(), reward, done, {}\n    \n    def _process_other_bidders(self):\n        \"\"\"Process the bidding behavior of other bidders.\"\"\"\n        # Process snipers\n        for sniper in self.snipers:\n            if not sniper['has_bid'] and self.current_time_step >= sniper['snipe_time']:\n                # Sniper places a bid if they value the item more than current price\n                if sniper['value'] > self.current_price:\n                    # Bid just enough to win\n                    bid = min(sniper['value'], self.current_price + random.choice([1, 2, 5]))\n                    if bid > self.current_price:\n                        self.current_price = bid\n                        self.agent_is_winner = False\n                        self.last_bid_change = bid - self.current_price\n                sniper['has_bid'] = True\n        \n        # Process incrementalists\n        for inc in self.incrementalists:\n            # Check if it's time for this incrementalist to bid\n            if (self.current_time_step - inc['last_bid_time'] >= inc['bid_frequency']):\n                # Incrementalist places a bid if they value the item more than current price\n                if inc['value'] > self.current_price:\n                    bid = min(inc['value'], self.current_price + inc['bid_increment'])\n                    if bid > self.current_price:\n                        self.current_price = bid\n                        self.agent_is_winner = False\n                        self.last_bid_change = bid - self.current_price\n                inc['last_bid_time'] = self.current_time_step\n        \n        # Process jump bidders\n        for jumper in self.jump_bidders:\n            if not jumper['has_jumped'] and self.current_time_step == jumper['jump_time']:\n                # Jump bidder places a big bid if they value the item more than current price\n                if jumper['value'] > self.current_price:\n                    # Jump to a significant fraction of their value\n                    bid = min(jumper['value'], max(self.current_price + 1, jumper['value'] * jumper['jump_fraction']))\n                    if bid > self.current_price:\n                        self.current_price = bid\n                        self.agent_is_winner = False\n                        self.last_bid_change = bid - self.current_price\n                jumper['has_jumped'] = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- DEEP Q-NETWORK AGENT ----------\n\nclass QNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(QNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 64)\n        self.fc2 = nn.Linear(64, 64)\n        self.fc3 = nn.Linear(64, action_dim)\n    \n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99, \n                epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995,\n                memory_size=10000, batch_size=64):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.gamma = gamma  # discount factor\n        self.epsilon = epsilon  # exploration rate\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.learning_rate = learning_rate\n        self.memory = deque(maxlen=memory_size)\n        self.batch_size = batch_size\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Q-Network\n        self.q_network = QNetwork(state_dim, action_dim).to(self.device)\n        self.target_network = QNetwork(state_dim, action_dim).to(self.device)\n        self.target_network.load_state_dict(self.q_network.state_dict())\n        self.target_network.eval()\n        \n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n        \n        # For storing experiences\n        self.Transition = namedtuple('Transition', \n                                     ('state', 'action', 'next_state', 'reward', 'done'))\n    \n    def select_action(self, state):\n        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n        if random.random() < self.epsilon:\n            return random.randrange(self.action_dim)\n        else:\n            with torch.no_grad():\n                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n                q_values = self.q_network(state_tensor)\n                return q_values.argmax().item()\n    \n    def remember(self, state, action, next_state, reward, done):\n        \"\"\"Store experience in memory.\"\"\"\n        self.memory.append(self.Transition(state, action, next_state, reward, done))\n    \n    def replay(self):\n        \"\"\"Experience replay to update Q-network.\"\"\"\n        if len(self.memory) < self.batch_size:\n            return\n        \n        # Sample a mini-batch from memory\n        transitions = random.sample(self.memory, self.batch_size)\n        batch = self.Transition(*zip(*transitions))\n        \n        # Convert to PyTorch tensors\n        state_batch = torch.FloatTensor(np.array(batch.state)).to(self.device)\n        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1).to(self.device)\n        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(self.device)\n        done_batch = torch.FloatTensor(batch.done).unsqueeze(1).to(self.device)\n        \n        # Compute Q(s_t, a)\n        q_values = self.q_network(state_batch).gather(1, action_batch)\n        \n        # Compute max_a Q(s_{t+1}, a) for all next states\n        next_q_values = self.target_network(next_state_batch).max(1)[0].unsqueeze(1).detach()\n        \n        # Compute the expected Q values\n        expected_q_values = reward_batch + (self.gamma * next_q_values * (1 - done_batch))\n        \n        # Compute loss\n        loss = F.mse_loss(q_values, expected_q_values)\n        \n        # Optimize the model\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        # Decay exploration rate\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n    \n    def update_target_network(self):\n        \"\"\"Update the target network with the weights from the Q-network.\"\"\"\n        self.target_network.load_state_dict(self.q_network.state_dict())\n    \n    def save(self, filename):\n        \"\"\"Save the model.\"\"\"\n        torch.save({\n            'q_network_state_dict': self.q_network.state_dict(),\n            'target_network_state_dict': self.target_network.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n        }, filename)\n    \n    def load(self, filename):\n        \"\"\"Load the model.\"\"\"\n        checkpoint = torch.load(filename)\n        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# ---------- TRAINING LOOP ----------\n\ndef train(env, agent, episodes=1000, target_update=10, display_freq=100):\n    \"\"\"Train the agent in the environment.\"\"\"\n    rewards = []\n    wins = []\n    profits = []\n    \n    for episode in range(1, episodes + 1):\n        state = env.reset()\n        total_reward = 0\n        done = False\n        \n        while not done:\n            # Select an action\n            action = agent.select_action(state)\n            \n            # Take a step in the environment\n            next_state, reward, done, _ = env.step(action)\n            \n            # Store the experience\n            agent.remember(state, action, next_state, reward, done)\n            \n            # Update state\n            state = next_state\n            \n            # Accumulate reward\n            total_reward += reward\n            \n            # Perform experience replay\n            agent.replay()\n        \n        # Update the target network\n        if episode % target_update == 0:\n            agent.update_target_network()\n        \n        # Track metrics\n        rewards.append(total_reward)\n        wins.append(env.agent_is_winner)\n        if env.agent_is_winner:\n            profits.append(env.agent_private_value - env.current_price)\n        else:\n            profits.append(0)\n        \n        # Display progress\n        if episode % display_freq == 0:\n            win_rate = sum(wins[-display_freq:]) / display_freq\n            avg_reward = sum(rewards[-display_freq:]) / display_freq\n            avg_profit = sum(profits[-display_freq:]) / display_freq\n            print(f\"Episode: {episode}, Win Rate: {win_rate:.2f}, Avg Reward: {avg_reward:.2f}, Avg Profit: {avg_profit:.2f}, Epsilon: {agent.epsilon:.2f}\")\n    \n    return rewards, wins, profits\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ---------- EVALUATION ----------\n\ndef evaluate(env, agent, episodes=100):\n    \"\"\"Evaluate the trained agent.\"\"\"\n    rewards = []\n    wins = []\n    profits = []\n    bid_times = []  # When the agent bids\n    bid_amounts = []  # How much the agent bids\n    \n    for episode in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        done = False\n        episode_bid_times = []\n        episode_bid_amounts = []\n        \n        while not done:\n            # Select an action (no exploration during evaluation)\n            with torch.no_grad():\n                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n                q_values = agent.q_network(state_tensor)\n                action = q_values.argmax().item()\n            \n            # Record bid if agent places one\n            if env.actions[action] > 0:\n                episode_bid_times.append(env.current_time_step / env.max_time_steps)\n                episode_bid_amounts.append(env.actions[action])\n            \n            # Take a step in the environment\n            next_state, reward, done, _ = env.step(action)\n            \n            # Update state\n            state = next_state\n            \n            # Accumulate reward\n            total_reward += reward\n        \n        # Track metrics\n        rewards.append(total_reward)\n        wins.append(env.agent_is_winner)\n        if env.agent_is_winner:\n            profits.append(env.agent_private_value - env.current_price)\n        else:\n            profits.append(0)\n        \n        bid_times.append(episode_bid_times)\n        bid_amounts.append(episode_bid_amounts)\n    \n    win_rate = sum(wins) / episodes\n    avg_reward = sum(rewards) / episodes\n    avg_profit = sum([p for p in profits if p > 0]) / (sum(wins) if sum(wins) > 0 else 1)\n    \n    print(f\"Evaluation Results:\")\n    print(f\"Win Rate: {win_rate:.2f}\")\n    print(f\"Average Reward: {avg_reward:.2f}\")\n    print(f\"Average Profit (when winning): {avg_profit:.2f}\")\n    \n    return rewards, wins, profits, bid_times, bid_amounts\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ---------- VISUALIZATION ----------\n\ndef plot_training_metrics(rewards, wins, profits):\n    \"\"\"Plot training metrics.\"\"\"\n    plt.figure(figsize=(15, 5))\n    \n    # Plot average reward over episodes\n    plt.subplot(1, 3, 1)\n    plt.plot(rewards)\n    plt.title('Average Reward per Episode')\n    plt.xlabel('Episode')\n    plt.ylabel('Reward')\n    \n    # Plot win rate\n    plt.subplot(1, 3, 2)\n    window_size = 100\n    win_rate = [sum(wins[i:i+window_size])/window_size for i in range(0, len(wins)-window_size+1)]\n    plt.plot(range(window_size, len(wins)+1), win_rate)\n    plt.title(f'Win Rate (Moving Average, Window={window_size})')\n    plt.xlabel('Episode')\n    plt.ylabel('Win Rate')\n    \n    # Plot average profit\n    plt.subplot(1, 3, 3)\n    non_zero_profits = [(i, p) for i, p in enumerate(profits) if p > 0]\n    if non_zero_profits:\n        indices, values = zip(*non_zero_profits)\n        plt.scatter(indices, values, alpha=0.5)\n        plt.title('Profit When Winning')\n        plt.xlabel('Episode')\n        plt.ylabel('Profit')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_bidding_behavior(bid_times, bid_amounts):\n    \"\"\"Plot the agent's bidding behavior.\"\"\"\n    plt.figure(figsize=(15, 6))\n    \n    # Flatten the lists\n    all_times = [time for episode_times in bid_times for time in episode_times]\n    all_amounts = [amount for episode_amounts in bid_amounts for amount in episode_amounts]\n    \n    if not all_times:  # Check if the agent placed any bids\n        plt.text(0.5, 0.5, 'No bids placed by the agent', \n                 horizontalalignment='center', verticalalignment='center')\n    else:\n        # Create a scatter plot of bid times vs. bid amounts\n        plt.scatter(all_times, all_amounts, alpha=0.5)\n        plt.title('Agent Bidding Behavior')\n        plt.xlabel('Normalized Time in Auction')\n        plt.ylabel('Bid Increment Amount')\n        \n        # Add a histogram to show the distribution of bid times\n        plt.figure(figsize=(15, 6))\n        plt.hist(all_times, bins=20)\n        plt.title('Distribution of Agent Bid Timings')\n        plt.xlabel('Normalized Time in Auction')\n        plt.ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef evaluate_against_specific_opponents(agent, episodes=100):\n    \"\"\"Evaluate the agent against specific opponent compositions.\"\"\"\n    scenarios = [\n        {\"name\": \"Only Snipers\", \"snipers\": 3, \"incrementalists\": 0, \"jump_bidders\": 0},\n        {\"name\": \"Only Incrementalists\", \"snipers\": 0, \"incrementalists\": 3, \"jump_bidders\": 0},\n        {\"name\": \"Only Jump Bidders\", \"snipers\": 0, \"incrementalists\": 0, \"jump_bidders\": 3},\n        {\"name\": \"Mixed (1 each)\", \"snipers\": 1, \"incrementalists\": 1, \"jump_bidders\": 1},\n    ]\n    \n    results = {}\n    \n    for scenario in scenarios:\n        print(f\"\\nEvaluating against {scenario['name']}...\")\n        \n        # Create a specific environment\n        env = AuctionEnv(\n            n_snipers=scenario[\"snipers\"],\n            n_incrementalists=scenario[\"incrementalists\"],\n            n_jump_bidders=scenario[\"jump_bidders\"]\n        )\n        \n        # Evaluate\n        rewards, wins, profits, _, _ = evaluate(env, agent, episodes)\n        \n        win_rate = sum(wins) / episodes\n        avg_reward = sum(rewards) / episodes\n        avg_profit = sum([p for p in profits if p > 0]) / (sum(wins) if sum(wins) > 0 else 1)\n        \n        results[scenario[\"name\"]] = {\n            \"win_rate\": win_rate,\n            \"avg_reward\": avg_reward,\n            \"avg_profit\": avg_profit\n        }\n    \n    # Display results\n    print(\"\\nComparison Against Different Opponent Types:\")\n    for name, metrics in results.items():\n        print(f\"{name}:\")\n        print(f\"  Win Rate: {metrics['win_rate']:.2f}\")\n        print(f\"  Avg Reward: {metrics['avg_reward']:.2f}\")\n        print(f\"  Avg Profit (when winning): {metrics['avg_profit']:.2f}\")\n    \n    return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- MAIN FUNCTION ----------\n\ndef main():\n    # Create the auction environment\n    env = AuctionEnv(\n        max_time_steps=100,\n        min_item_value=50,\n        max_item_value=200,\n        n_snipers=1,\n        n_incrementalists=1,\n        n_jump_bidders=1\n    )\n    \n    # Create the DQN agent\n    agent = DQNAgent(\n        state_dim=env.state_dim,\n        action_dim=env.action_space,\n        learning_rate=0.001,\n        gamma=0.99,\n        epsilon=1.0,\n        epsilon_min=0.01,\n        epsilon_decay=0.995,\n        memory_size=10000,\n        batch_size=64\n    )\n    \n    # Train the agent\n    print(\"Starting training...\")\n    rewards, wins, profits = train(env, agent, episodes=5000, target_update=10, display_freq=100)\n    \n    # Save the trained model\n    agent.save(\"auction_bidder_dqn.pth\")\n    \n    # Plot training metrics\n    plot_training_metrics(rewards, wins, profits)\n    \n    # Evaluate the agent\n    print(\"\\nEvaluating the agent...\")\n    eval_rewards, eval_wins, eval_profits, bid_times, bid_amounts = evaluate(env, agent, episodes=100)\n    \n    # Plot bidding behavior\n    plot_bidding_behavior(bid_times, bid_amounts)\n    \n    # Evaluate against specific opponent compositions\n    results = evaluate_against_specific_opponents(agent)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}